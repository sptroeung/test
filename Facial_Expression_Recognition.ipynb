{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Facial Expression Recognition Competition (15%)**\n",
        "For this competition, we will use the a facial classification(https://cloudstor.aarnet.edu.au/plus/s/8J44RsLu7uyRzhd) dataset. The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image. You can download the CSV from this link (https://drive.google.com/file/d/1B_3ABybPrJKSkGJNSSJwctQijYOHcJZu/view)\n",
        "\n",
        "The task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0: Angry, 1: Disgust, 2: Fear, 3: Happy, 4: Sad, 5: Surprise, 6: Neutral). The training set consists of 28,709 examples and the public test set consists of 3,589 examples.\n",
        "\n",
        "We provide baseline code that includes the following features:\n",
        "\n",
        "*   Loding and Analysing the FER-2013 dataset using torchvision.\n",
        "*   Defining a simple convolutional neural network. \n",
        "*   How to use existing loss function for the model learning. \n",
        "*   Train the network on the training data. \n",
        "*   Test the trained network on the testing data. \n",
        "*   Generate prediction for the random test image(s). \n",
        "\n",
        "The following changes could be considered:\n",
        "-------\n",
        "1. Change of advanced training parameters: Learning Rate, Optimizer, Batch-size, Number of Max Epochs, and Drop-out. \n",
        "2. Use of a new loss function.\n",
        "3. Data augmentation\n",
        "4. Architectural Changes: Batch Normalization, Residual layers, Attention Block, and other varients.\n",
        "\n",
        "Marking Rules:\n",
        "-------\n",
        "We will mark the competition based on the final test accuracy on testing images and your report.\n",
        "\n",
        "Final mark (out of 50) = acc_mark + efficiency mark + report mark\n",
        "###Acc_mark 10:\n",
        "\n",
        "We will rank all the submission results based on their test accuracy. Zero improvement over the baseline yields 0 marks. Maximum improvement over the baseline will yield 10 marks. There will be a sliding scale applied in between.\n",
        "\n",
        "###Efficiency mark 10:\n",
        "\n",
        "Efficiency considers not only the accuracy, but the computational cost of running the model (flops: https://en.wikipedia.org/wiki/FLOPS). Efficiency for our purposes is defined to be the ratio of accuracy (in %) to Gflops. Please report the computational cost for your final model and include the efficiency calculation in your report. Maximum improvement over the baseline will yield 10 marks. Zero improvement over the baseline yields zero marks, with a sliding scale in between.\n",
        "\n",
        "###Report mark 30:\n",
        "Your report should comprise:\n",
        "1. An introduction showing your understanding of the task and of the baseline model: [10 marks]\n",
        "\n",
        "2. A description of how you have modified aspects of the system to improve performance. [10 marks]\n",
        "\n",
        "A recommended way to present a summary of this is via an \"ablation study\" table, eg:\n",
        "\n",
        "|Method1|Method2|Method3|Accuracy|\n",
        "|---|---|---|---|\n",
        "|N|N|N|60%|\n",
        "|Y|N|N|65%|\n",
        "|Y|Y|N|77%|\n",
        "|Y|Y|Y|82%|\n",
        "\n",
        "3. Explanation of the methods for reducing the computational cost and/or improve the trade-off between accuracy and cost: [5 marks]\n",
        "\n",
        "4. Limitations/Conclusions: [5 marks] \n"
      ],
      "metadata": {
        "id": "PyDkfc31TTAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################################################################################\n",
        "### Subject: Computer Vision \n",
        "### Year: 2023\n",
        "### Student Name: ABC, XYZ\n",
        "### Student ID: a123456, a654321\n",
        "### Comptetion Name: Facial Expression Recognition/Classification\n",
        "### Final Results:\n",
        "### ACC:         FLOPs:\n",
        "##################################################################################################################################"
      ],
      "metadata": {
        "id": "_zFmch0IO4S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx1KHLh6TOFY"
      },
      "outputs": [],
      "source": [
        "# Importing libraries. \n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# To avoid non-essential warnings \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline \n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting G-Drive to get your dataset. \n",
        "# To access Google Colab GPU; Go To: Edit >>> Network Settings >>> Hardware Accelarator: Select GPU. \n",
        "# Reference: https://towardsdatascience.com/google-colab-import-and-export-datasets-eccf801e2971 \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Dataset path.\n",
        "data_directory ='/content/drive/MyDrive/Datasets/fer2013/fer2013.csv'"
      ],
      "metadata": {
        "id": "da5uVjJKThYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset file using Pandas read_csv function and print the first\n",
        "# 5 samples. \n",
        "#\n",
        "# Reference: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
        "data_df = pd.read_csv(data_directory)\n",
        "data_df.head(4)"
      ],
      "metadata": {
        "id": "Q1842ueQTxnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping of the Facial Expression Labels. \n",
        "Labels = {\n",
        "    0:'Angry',\n",
        "    1:'Disgust',\n",
        "    2:'Fear',\n",
        "    3:'Happy',\n",
        "    4:'Sad',\n",
        "    5:'Surprise',\n",
        "    6:'Neutral'\n",
        "}\n",
        "Labels"
      ],
      "metadata": {
        "id": "sUsJYHt0T6yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorizing the dataset to three categories. \n",
        "# Training: To train the model.\n",
        "# PrivateTest: To test the train model; commonly known as Validation. \n",
        "# PublicTest: To test the final model on Test set to check how your model perfomed. Do not use this data as your validation data. \n",
        "train_df = data_df[data_df['Usage']=='Training']\n",
        "valid_df = data_df[data_df['Usage']=='PublicTest']\n",
        "test_df = data_df[data_df['Usage']=='PrivateTest']\n",
        "print(train_df.head())\n",
        "print(valid_df.head(-1))"
      ],
      "metadata": {
        "id": "L-7qJKDkT9Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test-check to see wether usage labels have been allocated to the dataset/not.\n",
        "valid_df = valid_df.reset_index(drop=True) \n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(test_df.head())\n",
        "print('   -----   -------    -------    --------     -----    -------')\n",
        "print(valid_df.head())"
      ],
      "metadata": {
        "id": "8QjeGOPqT_Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview of the training sample and associated labels.\n",
        "def show_example(df, num):\n",
        "    print('expression: ' ,df.iloc[num] )\n",
        "    image = np.array([[int(i) for i in x.split()] for x in df.loc[num, ['pixels']]])\n",
        "    print(image.shape)\n",
        "    image = image.reshape(48,48)\n",
        "    plt.imshow(image, interpolation='nearest', cmap='gray')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "r-xYa-jlUBIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_example(train_df, 107)"
      ],
      "metadata": {
        "id": "AZPA0K3XYaql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization of the train and validation data.\n",
        "class expressions(Dataset):\n",
        "    def __init__(self, df, transforms=None):\n",
        "        self.df = df\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.loc[index]\n",
        "        image, label = np.array([x.split() for x in self.df.loc[index, ['pixels']]]), row['emotion']\n",
        "        #image = image.reshape(1,48,48)\n",
        "        image = np.asarray(image).astype(np.uint8).reshape(48,48,1)\n",
        "        #image = np.reshape(image,(1,48,48))\n",
        "       \n",
        "        \n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return image.clone().detach(), label"
      ],
      "metadata": {
        "id": "ShizdbgsUdTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import albumentations as A\n",
        "stats = ([0.5],[0.5])"
      ],
      "metadata": {
        "id": "8_bY6dp9UgIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tsfm = T.Compose([   \n",
        "    T.ToPILImage(),\n",
        "    T.Grayscale(num_output_channels=1),\n",
        "    T.ToTensor(), \n",
        "    T.Normalize(*stats,inplace=True), \n",
        "])\n",
        "valid_tsfm = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Grayscale(num_output_channels=1),\n",
        "    T.ToTensor(), \n",
        "    T.Normalize(*stats,inplace=True)\n",
        "])"
      ],
      "metadata": {
        "id": "tY-VDCynUicG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = expressions(train_df, train_tsfm)\n",
        "valid_ds = expressions(valid_df, valid_tsfm)\n",
        "test_ds = expressions(test_df, valid_tsfm)"
      ],
      "metadata": {
        "id": "b1POsgNqUkM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 400\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, \n",
        "                      num_workers=2, pin_memory=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size*2, \n",
        "                    num_workers=2, pin_memory=True)\n",
        "test_dl = DataLoader(test_ds, batch_size*2, \n",
        "                    num_workers=2, pin_memory=True)\n",
        "def show_batch(dl):\n",
        "    for images, labels in dl:\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(images, nrow=20).permute(1, 2, 0))\n",
        "        break\n",
        "        \n",
        "show_batch(train_dl)"
      ],
      "metadata": {
        "id": "FIPuknSBUnh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metric - Accuracy in this case. \n",
        "\n",
        "import torch.nn.functional as F\n",
        "input_size = 48*48\n",
        "output_size = len(Labels)\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    predictions, preds = torch.max(output, dim=1)\n",
        "    return torch.tensor(torch.sum(preds==labels).item()/len(preds))"
      ],
      "metadata": {
        "id": "g2zV2cuDUxFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expression model class for training and validation purpose. \n",
        "\n",
        "class expression_model(nn.Module):\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        acc = accuracy(out, labels)\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "    \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()\n",
        "        batch_acc = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_acc).mean()\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch[{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
      ],
      "metadata": {
        "id": "Qvl3olxrU2W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check wether Google Colab GPU has been assigned/not. \n",
        "torch.cuda.is_available()\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return None\n",
        "device = get_default_device()\n",
        "device"
      ],
      "metadata": {
        "id": "9dWl9SByU-wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "XBiFaKAKVEo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "valid_dl = DeviceDataLoader(valid_dl, device)\n",
        "test_dl = DeviceDataLoader(test_dl, device)"
      ],
      "metadata": {
        "id": "mPBWSiPkeyKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic model - 1 layer\n",
        "simple_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\n",
        "    nn.MaxPool2d(2, 2)\n",
        ")"
      ],
      "metadata": {
        "id": "_WYYWRLKe1QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_dl:\n",
        "    print('images.shape:', images.shape)\n",
        "    out = simple_model(images)\n",
        "    print('out.shape:', out.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "IJouDbTIe3vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model - 7 layer\n",
        "class expression(expression_model):\n",
        "    def __init__(self,classes):\n",
        "        super().__init__()\n",
        "        self.num_classes = classes\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),  #(input channels, output channels)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 32, kernel_size=3, padding=1),  #(input channels, output channels)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 64 x 24 x 24\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 12 x 12\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 6 x 6\n",
        "\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(256*6*6, 2304),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2304, 1152),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1152, 576),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(576,288),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(288,144),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(144,self.num_classes))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ],
      "metadata": {
        "id": "B5dmt4X6VO-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model print\n",
        "model = to_device(expression(classes = 7), device)\n",
        "model"
      ],
      "metadata": {
        "id": "ceJQtRq1VRAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for evaluation and training.\n",
        "@torch.no_grad()\n",
        "def evaluate(model, valid_dl):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in valid_dl]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_dl, valid_dl, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_dl:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, valid_dl)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "metadata": {
        "id": "8ms49hovVTSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, valid_dl)"
      ],
      "metadata": {
        "id": "gMhOvBlsd9ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "opt_func = torch.optim.Adam\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "6cSs_ji4Vbvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = fit(num_epochs, lr, model, train_dl, valid_dl, opt_func)"
      ],
      "metadata": {
        "id": "If7zRxdjVf80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots for accuracy and loss during training period. \n",
        "def plot_accuracies(history):\n",
        "    accuracies = [x['val_acc'] for x in history]\n",
        "    plt.plot(accuracies, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title('Accuracy vs. No. of epochs');\n",
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs');"
      ],
      "metadata": {
        "id": "Qi-kLqDYVil_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_accuracies(history)"
      ],
      "metadata": {
        "id": "oWslwBg5VnQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(history)"
      ],
      "metadata": {
        "id": "qOHN8Yr0Vo4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation on test data.\n",
        "result = evaluate(model, test_dl)\n",
        "result"
      ],
      "metadata": {
        "id": "a8gl2x4fVsLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction function to evaluate the model. \n",
        "def predict_image(img, model):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "    return Labels[preds[0].item()]"
      ],
      "metadata": {
        "id": "28zn6GWwVwrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = test_ds[0]\n",
        "plt.imshow(img[0], interpolation='nearest', cmap='gray')\n",
        "img = img.to(device)\n",
        "print('Label:', Labels[label], ', Predicted:', predict_image(img, model))"
      ],
      "metadata": {
        "id": "XIFbEoS-V3MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = test_ds[110]\n",
        "plt.imshow(img[0], interpolation='nearest', cmap='gray')\n",
        "img = img.to(device)\n",
        "print('Label:', Labels[label], ', Predicted:', predict_image(img, model))"
      ],
      "metadata": {
        "id": "i88tAsWWV2ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FLOPs"
      ],
      "metadata": {
        "id": "cDrJajSvgax-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  #The code from https://cloudstor.aarnet.edu.au/plus/s/PcSc67ZncTSQP0E can be used to count flops\n",
        "  #Download the code.\n",
        "  !wget -c https://cloudstor.aarnet.edu.au/plus/s/hXo1dK9SZqiEVn9/download\n",
        "  !mv download FLOPs_counter.py\n",
        "  #!rm -rf download"
      ],
      "metadata": {
        "id": "6XzHrJOUgag9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from FLOPs_counter import print_model_parm_flops\n",
        "input = torch.randn(1, 1, 48, 48) # The input size should be the same as the size that you put into your model \n",
        "#Get the network and its FLOPs\n",
        "num_classes = 7\n",
        "model = expression(num_classes)\n",
        "print_model_parm_flops(model, input, detail=False)"
      ],
      "metadata": {
        "id": "5C3lwJgQgsdt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}