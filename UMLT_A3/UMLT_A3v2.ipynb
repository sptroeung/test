{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ebe399",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Sign Language Image Classification using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae8032",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this assignment you will implement different deep learning networks to classify images of hands in poses that correspond to letters in American Sign Language. The dataset is contained in the assignment zip file, along with some images and a text file describing the dataset. It is similar in many ways to other MNIST datasets.\n",
    "\n",
    "The American Sign Language letter database of hand gestures represent a multi-class problem with 24 classes of letters (**excluding J and Z which require motion**). The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and **no cases for 9=J or 25=Z because of gesture motions**). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, $pixel_{1}$,$pixel_{2}$â€¦.$pixel_{784}$ which represent a single 28x28 pixel image with grayscale values between 0-255.\n",
    "\n",
    "## Scenario\n",
    "\n",
    "A client is interested in having you (or rather the company that you work for) investigate whether it is possible to develop an app that would enable American sign language to be translated for people that do not sign, or those that sign in different languages/styles. They have provided you with a labelled data of images related to signs (hand positions) that represent individual letters in order to do a preliminary test of feasibility.\n",
    "\n",
    "Your manager has asked you to do this feasibility assessment, but subject to a constraint on the computational facilities available.  More specifically, you are asked to do **no more than 100 training runs in total** (including all models and hyperparameter settings that you consider).  \n",
    "\n",
    "The task requires you to create a Jupyter Notebook to perform 22 steps. These steps involve loading the dataset, fixing data problems, converting labels to one-hot encoding, plotting sample images, creating, training, and evaluating two sequential models with 20 Dense layers with 100 neurons each, checking for better accuracy using MC Dropout, retraining the first model with performance scheduling, evaluating both models, using transfer learning to create a new model using pre-trained weights, freezing the weights of the pre-trained layers, adding new Dense layers, training and evaluating the new model, predicting and converting sign language to text using the best model.\n",
    "\n",
    "### IMPORTANT\n",
    "* Train all the models locally on your own machine. No model training should occur on Gradescope (GS).\n",
    "* After completing the training, upload the trained models' **h5 files** and their training histories along with your notebook to GS.\n",
    "    * best_dnn_bn_model.h5\n",
    "    * best_dnn_bn_perf_model.h5\n",
    "    * best_dnn_selu_model.h5\n",
    "    * best_mobilenet_model.h5\n",
    "    * history1\n",
    "    * history2\n",
    "    * history1_perf\n",
    "    * historymb\n",
    "* To avoid any confusion and poor training on GS, please remember to comment out the training code in your notebook before uploading it to GS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601d848a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries (TensorFlow, sklearn NumPy, Pandas, and Matplotlib)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Install opencv using \"pip install opencv-python\" in order to use cv2.\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65792dc",
   "metadata": {},
   "source": [
    "**Step0** This test is for checking whether all the required files are submitted. Once you submit all the required files to the autograder, you will be able to pass this step.\n",
    "\n",
    "**IMPORTANT:** Run this step to determine whether you have created all the required files correctly.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f4275e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One or more files are missing!\n"
     ]
    }
   ],
   "source": [
    "# Don't change this cell code\n",
    "required_files = ['best_dnn_bn_model.h5','best_dnn_bn_perf_model.h5','best_dnn_selu_model.h5','best_mobilenet_model.h5','history1','history2','history1_perf','historymb']\n",
    "step0_files = True\n",
    "for file in required_files:\n",
    "    if os.path.exists(file) == False:\n",
    "        step0_files = False\n",
    "        print(\"One or more files are missing!\")\n",
    "        break\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.   \n",
    "step0_data = step0_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a08de-5250-460c-b589-4f2e386711d6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b498c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd664f",
   "metadata": {},
   "source": [
    "**STEP1** Load the dataset (train and test) using `Pandas` from the CSV file.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0322a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset using Pandas from the CSV file\n",
    "train_df = pd.read_csv(\"C:/Users/sptro/OneDrive/Documents/Jupyter Notebook/Machine Learning/Assignment 3/Sign_MNIST_Dataset/sign_mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"C:/Users/sptro/OneDrive/Documents/Jupyter Notebook/Machine Learning/Assignment 3/Sign_MNIST_Dataset/sign_mnist_test.csv\")\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step1_sol = test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724c552-18f8-479b-a08f-56b46b3ab840",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c0584",
   "metadata": {},
   "source": [
    "**STEP2** Examine the data and fix any problems. It is important that you don't have gaps in the number of classes, therefore, check the classes which are not available and shift the labels in order to ensure 24 classes, starting from class 0. In addition, normalize the values of your images in a range of 0 and 1.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488a9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate labels and pixel values in training and testing sets\n",
    "train_labels = ...\n",
    "test_labels = ...\n",
    "train_images = ...\n",
    "test_images = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step2_sol = (np.max(train_images)-np.min(train_images), np.max(train_labels)-np.min(train_labels), np.max(test_images)-np.min(test_images), np.max(test_labels)-np.min(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8ae39-75bc-40e0-8b49-7a47212cfcb6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d5816",
   "metadata": {},
   "source": [
    "**STEP3** Convert Labels to One-Hot Encoding both train and test.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a46b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "train_labels_encoded = ...\n",
    "test_labels_encoded = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step3_sol = (len(np.unique(train_labels_encoded)), len(np.unique(test_labels_encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b56f7-0494-43e5-b808-d6f9a9722675",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570c487",
   "metadata": {},
   "source": [
    "**STEP4** Plot one sample image for each letter in the dataset given in the training set. To solve this step you should use the function `imshow` to diplay your images in a similar to the image below.\n",
    "\n",
    "<center><img src=\"example_letters.jpg\" width=400 height=300/></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead36f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get one sample image for each label\n",
    "\n",
    "# Plot the sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d9987-8380-4ade-89bb-9f154b2a9b6b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a6b30",
   "metadata": {},
   "source": [
    "## Create Neural Network Architectures\n",
    "\n",
    "In this part you should create two different models (model1 and model2) with different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34849be2",
   "metadata": {},
   "source": [
    "**STEP5** \n",
    "Create one sequential model in `TensorFlow` with 20 Dense layers with 100 neurons each one. Consider the specific modifications that you need to do in order to work with your specific input and those to get the required output. \n",
    "\n",
    "* This model uses Batch Normalization in each layer and uses He initialization. Add Swish activation function to each layer.\n",
    "\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76f769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.activations import swish\n",
    "from tensorflow.keras.layers import InputLayer, AlphaDropout\n",
    "\n",
    "# Model 1: with Batch Normalization\n",
    "model1 = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step5_sol = model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f4031-4de3-4de5-aef5-21e1d60ed456",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cffa84",
   "metadata": {},
   "source": [
    "**STEP6** \n",
    "Create a second sequential model in `TensorFlow` also with 20 Dense layers with 100 neurons each one. Consider the specific modifications that you need to do in order to work with your specific input and those to get the required output.\n",
    "\n",
    "In this model you should:\n",
    "* Replace Batch Normalization with SELU and make necessary adjustments to ensure the network self-normalizes, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers. \n",
    "* Regularize this model with Alpha Dropout 0.1.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59dcc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model 2: with SELU and self-normalization\n",
    "model2 = ...\n",
    "    ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step6_sol = model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae33570-c5fe-4882-8faf-ba50490f18c2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d33d4",
   "metadata": {},
   "source": [
    "## Compile the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcee2f",
   "metadata": {},
   "source": [
    "**STEP7** \n",
    "Compile the both previous models using **Nadam** optimization. Also:\n",
    "\n",
    "* Set the loss function to categorical cross-entropy. \n",
    "* Set the metric to accuracy.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2df62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile first model\n",
    "\n",
    "# Compile second model\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step7_sol = (model1.loss,model2.loss,model1.optimizer.get_config()['name'],model2.optimizer.get_config()['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be248f-e1e3-4147-ae4e-f2e216f079b0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3097d06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64585c5",
   "metadata": {},
   "source": [
    "**STEP8** One of these models work preferably with data, which follow a normal distribution. Generate **train_images_scaled** and **test_images_scaled** using a `Sklearn` function that allow you to convert data to a normal distribution with mean 0 and variance equal 1. \n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6f980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_images_scaled = ...\n",
    "test_images_scaled = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step8_sol = (np.mean(train_images_scaled),np.mean(test_images_scaled),np.std(train_images_scaled),np.std(test_images_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bc6ca-9487-496a-a545-9179af460b3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step08\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991bf934",
   "metadata": {},
   "source": [
    "**STEP9** Train the two models on the training dataset using early stopping. In order to save the results given by your training for your models, create checkpoints saving the best model in each case using the function `ModelCheckpoint`. Note that one of the models use the scaled data obtained in **STEP8**. Meanwhile, the other model does not. Figure out which is the proper input data for each model.\n",
    "\n",
    "* Limit the number of epochs to 100. Set the batch size to or greater than 32.\n",
    "\n",
    "**IMPORTANT:** Comment out the code to train/fit the two models. Keep the code given to save the models.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a04d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# The following line of code is used by the autograder. Do not modify it.\n",
    "history1, history2 = None, None\n",
    " \n",
    "# We go to set a seed to get the same results every run\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = ...\n",
    "# Define model checkpoint callback\n",
    "model1_checkpoint_cb = ...\n",
    "model2_checkpoint_cb = ...\n",
    "\n",
    "#################################### Perform the training on your machine and then comment out the following section before uploading it to gradescope. \n",
    "# Make sure your best model is named as follows:\n",
    "# Model1 filename = best_dnn_bn_model.h5\n",
    "# Model2 filename = best_dnn_selu_model.h5\n",
    "\n",
    "# Train model 1 (Comment this out before submission)\n",
    "history1 = ...\n",
    "\n",
    "# Train model 2 (Comment this out before submission)\n",
    "history2 = ...\n",
    "\n",
    "# The following code will save your history - don't change it.\n",
    "if 'history1' in globals():  \n",
    "    with open('./history1', 'wb') as file_pi:\n",
    "        pickle.dump(history1.history, file_pi)\n",
    "if 'history2' in globals():  \n",
    "    with open('./history2', 'wb') as file_pi:\n",
    "        pickle.dump(history2.history, file_pi)\n",
    "####################################\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step9_sol = (model1_checkpoint_cb, model2_checkpoint_cb, early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3db5e-2ac5-449e-abe5-1e7c861d8859",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step09\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891cbbe",
   "metadata": {},
   "source": [
    "**STEP10** Using the loaded models obtained from the code given below, evaluate the performance of each model on the test set. While evaluating, make sure to take into account which model uses scaled data and which model does not.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc4f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not change the following 4 lines of code.\n",
    "# define the file name for the saved model\n",
    "model1_name = \"best_dnn_bn_model.h5\"\n",
    "# load the model\n",
    "model1 = keras.models.load_model(model1_name)\n",
    "# define the file name for the saved model\n",
    "model2_name = \"best_dnn_selu_model.h5\"\n",
    "# load the model\n",
    "model2 = keras.models.load_model(model2_name)\n",
    "\n",
    "# Evaluate the models on the test set\n",
    "test_loss1, test_acc1 = ...\n",
    "test_loss2, test_acc2 = ...\n",
    "print(f\"Model 1| Test accuracy: {test_acc1:.4f}, Test loss: {test_loss1:.4f}\")\n",
    "print(f\"Model 2| Test accuracy: {test_acc2:.4f}, Test loss: {test_loss2:.4f}\")\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step10_sol = (test_loss1, test_acc1, test_loss2, test_acc2, model1, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c385336-3e73-4d81-9862-ca43d55d943d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cdacb",
   "metadata": {},
   "source": [
    "**STEP11** From the loaded history of the two trained models, plot a graph of **accuracy** vs **number of epochs** for both training and validation. \n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d962c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load history for model 1 (Do not modify this code)\n",
    "history_name1 = \"./history1\"\n",
    "with open(history_name1, \"rb\") as file_pi:\n",
    "    loaded_history1 = pickle.load(file_pi)\n",
    "    \n",
    "# Load history for model 2 (Do not modify this code)\n",
    "history_name2 = \"./history2\"\n",
    "with open(history_name2, \"rb\") as file_pi:\n",
    "    loaded_history2 = pickle.load(file_pi)\n",
    "\n",
    "# Plot the training and validation accuracies during training for both models\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step11_sol = (loaded_history1, loaded_history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97812ff0-bcb9-441f-a2ec-487a938bb564",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c738d",
   "metadata": {},
   "source": [
    "## MC Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ebe13",
   "metadata": {},
   "source": [
    "**STEP12** Check if model2 achieves better accuracy using MC Dropout (without retraining).\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75540f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function computes the MC (Monte Carlo) Dropout predictions for a given model and input data. \n",
    "# It returns the mean of multiple predictions obtained by running the model in training mode.\n",
    "#Parameters\n",
    "#   model: A trained model with a dropout layers.\n",
    "#   X    : The input data for which the predictions are to be made.\n",
    "#   n_samples: The number of Monte Carlo samples to generate..\n",
    "#Returns\n",
    "  # The function returns an array-like object containing the MC Dropout predictions for the given input data. \n",
    "  # The shape of the output should be the same as the model's output layer.\n",
    "def mc_dropout_predict(model, X, n_samples=20):\n",
    "    # Write your code here\n",
    "    return output_mc\n",
    "\n",
    "# call mc_dropout_predict to Compute the MC Dropout predictions for model 2\n",
    "output_mc = ...\n",
    "\n",
    "# Compute the accuracy using MC Dropout\n",
    "accuracy_mc = ...\n",
    "\n",
    "# Display result.\n",
    "print(f\"Model 2 with MC Dropout: Test accuracy: {accuracy_mc:.4f}\")\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step12_sol = (output_mc, accuracy_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622f00e-50a9-4464-8779-8c41900dea68",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8c9246",
   "metadata": {},
   "source": [
    "## Learning Rate (LR) scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ce052",
   "metadata": {},
   "source": [
    "**STEP13** Retrain model1 using performance scheduling and see if it improves training speed and model accuracy.\n",
    "\n",
    "**IMPORTANT:** Define the model the same way model1 was defined and compile the model the same way as model1.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7842b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Do not modify the following line of code.\n",
    "history1_perf = None\n",
    "\n",
    "# Redefine Model 1 so we start again with random weights\n",
    "model1_perfLRS = ...\n",
    "\n",
    "# Compile the model with Nadam optimizer and categorical cross-entropy loss\n",
    "\n",
    "# Define the learning rate schedule\n",
    "lr_scheduler = ...\n",
    "\n",
    "# Creating model checkpoint to save the best model. \n",
    "dnn_bn_perf_checkpoint_cb = ...\n",
    "\n",
    "#####Perform the training on your machine and then comment out the following section before uploading it to gradescope.\n",
    "####################################  \n",
    "# Make sure your best model is named as follows:\n",
    "# Model1 with performance scheduling filename = best_dnn_bn_perf_model.h5\n",
    "\n",
    "\n",
    "# Train the model using early stopping and exponential scheduling (Comment this out before submission)\n",
    "history1_perf = ...\n",
    "\n",
    "# The following code will save your history - don't change it - comment it out before uploading to GS\n",
    "if \"history1_perf\" in globals():\n",
    "    with open('./history1_perf', 'wb') as file_pi:\n",
    "        pickle.dump(history1_perf.history, file_pi)\n",
    "####################################\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step13_sol = (dnn_bn_perf_checkpoint_cb, lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dda943-1e58-4273-a7d2-a5c448d0c9d5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0febb9",
   "metadata": {},
   "source": [
    "**STEP14** Using the loaded model obtained from the code given below, evaluate the performance of the model on the test set.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefaae3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not change the following 2 lines of code.\n",
    "# define the file name for the saved model\n",
    "model_name = \"best_dnn_bn_perf_model.h5\"\n",
    "# load the model\n",
    "model1_perf = keras.models.load_model(model_name)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss1_perf, test_acc1_perf = ...\n",
    "print(f\"Model 1 with performance scheduling: Test accuracy: {test_acc1_perf:.4f}\")\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step14_sol = (test_loss1_perf, test_acc1_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49855f84-5eb3-455e-93d9-6415cc664665",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6efa0",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**STEP15** From the history of the models loaded using the code given below, plot a graph of **accuracy** vs **number of epochs** for both training and validation.\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedea47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load history for model 1 with learning rate scheduling (do not modify the following code)\n",
    "history_name1_perf = \"./history1_perf\"\n",
    "with open(history_name1_perf, \"rb\") as file_pi:\n",
    "    loaded_history1_perf = pickle.load(file_pi)\n",
    "\n",
    "# Load history for the original model1 (do not modify the following code)\n",
    "history_name1 = \"./history1\"\n",
    "with open(history_name1, \"rb\") as file_pi:\n",
    "    loaded_history1 = pickle.load(file_pi)\n",
    "\n",
    "# Plot the training and validation accuracy for both models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ba41d-3eb6-4143-bb66-a3a97e42df04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db8f47",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Transfer learning\n",
    "\n",
    "Use transfer learning by using a pre-trained **MobileNetV3Small** model on imagenet dataset, and fine-tuning it on the Sign Language MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2df1b",
   "metadata": {},
   "source": [
    "**STEP16** First of all, you to prepare your data for this model.\n",
    "* Reshape your input data (train and test) to (28, 28, 3).\n",
    "* Standardize your input. \n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b58a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshape train data to (num_samples, 28, 28, 3)\n",
    "train_images_mb = ...\n",
    "\n",
    "# Reshape test data to (num_samples, 28, 28, 3)\n",
    "test_images_mb = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step16_sol = (train_images_mb,test_images_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1aaf12-f852-4b0b-b343-e9555181e6bf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd4f64e",
   "metadata": {},
   "source": [
    "**Step17** Now, we need to define and set up the model. For this you need to follow the next steps:\n",
    "\n",
    "* Load the pre-trained **MobileNetV3Small** model with weights from ImageNet.\n",
    "* Freeze the weights of the pretrained layers.\n",
    "* Modify the input layer to accept inputs of shape (28, 28, 3). \n",
    "* Also, add layer `UpSampling2D` to upscale the input by a factor of 2.\n",
    "\n",
    "Consider that maybe you need to adapt the default output.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaedce8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV3Small \n",
    "\n",
    "# Load the pre-trained MobileNet model with weights from ImageNet\n",
    "base_mb_model = ...\n",
    "\n",
    "\n",
    "# Create the new model\n",
    "final_mb_model = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step17_sol = final_mb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fa680-dd76-4302-b7bc-632d52e6ee55",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44c7bf",
   "metadata": {},
   "source": [
    "**Step18** Once we define the model and do the specific modifications to adjust to our data, we compile it.\n",
    "\n",
    "* Use a learning rate schedule that uses an exponential decay schedule.\n",
    "* Compile and train the model.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c558a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an initial learning rate                            \n",
    "initial_learning_rate = ...\n",
    "\n",
    "# Create the proper learning rate schedule\n",
    "lr_schedule = ...\n",
    "\n",
    "# Compile model\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step18_sol = (lr_schedule, final_mb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94d059-51ce-4948-a28a-99c0f6108699",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad90ecc",
   "metadata": {},
   "source": [
    "**Step19** Train the mobilenet model. Include early stopping in your training procedure.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae907a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not modify the following line of code.\n",
    "history_mb = None\n",
    "\n",
    "# Define model checkpoint callback (Do not modify this code).\n",
    "best_model_checkpoint = ...\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping_cb = ...\n",
    "\n",
    "## Perform the training on your machine and then comment out the following section before uploading it to gradescope. \n",
    "#################################### \n",
    "# make sure your best model is named as follow:\n",
    "# MobileNet model filename = best_mobilenet_model.h5\n",
    "# Train the model (comment this section out)\n",
    "history_mb = ...\n",
    "\n",
    "# The following code will save your history - don't change it\n",
    "if \"history_mb\" in globals():\n",
    "    with open('./historymb', 'wb') as file_pi:\n",
    "        pickle.dump(history_mb.history, file_pi)    \n",
    "####################################\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step19_sol = (best_model_checkpoint, early_stopping_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4f0bd-c69d-4589-8710-494768a56303",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d365dad",
   "metadata": {},
   "source": [
    "**Step20** For the trained model loaded using the code given below, evaluate its performance on the test set.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aab66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not modify the following two lines of code.\n",
    "# define the file name for the saved model\n",
    "model_name = \"best_mobilenet_model.h5\"\n",
    "# load the model\n",
    "final_mb_model = keras.models.load_model(model_name)\n",
    "\n",
    "# Reshape the input data to (num_samples, 28, 28, 3)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss1_mobilenet, test_acc1_mobilenet = ...\n",
    "print(f\"Model Mobile Net: Test accuracy: {test_acc1_mobilenet:.4f}\")\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step20_sol = (test_loss1_mobilenet,test_acc1_mobilenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294b6c3-07b4-442c-aaf5-6c78fc165f7c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c61a3",
   "metadata": {},
   "source": [
    "**Step21** So far, you have seen the overall performance of your models. However, it is possible that some classes may be more difficult to classify than others. To gain a clearer understanding of which letters are the most difficult or easiest to predict, you can use your MobileNet model and make predictions on your test data using the predict function. Based on this, you can check the proportion of correct matches for each letter over the total number of that specific letter in the test data (as the proportion of one letter may differ from that of others). Finally, return the result as a string indicating the most complex and easiest letter to predict based on our analysis (e.g., \"a\" in lowercase).\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01a672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put here again the labels (not hot encoded)\n",
    "test_labels = ...\n",
    "\n",
    "# Make the prediction using MobileNet model. Use the function predict.\n",
    "prediction_test = ...\n",
    "\n",
    "# What is the most difficult letter to predict? (if you have many letters which are equally difficult to predict, pick up any of them. Only one and put in a string (e.g. \"a\"))\n",
    "complex_letter = ...\n",
    "\n",
    "# What is the easist letter to predict? (if you have many letters which are equally easy to predict, pick up any of them. Only one and put in a string (e.g. \"a\"))\n",
    "easiest_letter = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step21_sol = (test_labels,prediction_test,complex_letter,easiest_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15871d90-0a9f-411f-9b9c-4c86aac10c7a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2084b",
   "metadata": {},
   "source": [
    "## Using our final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da3d3ee",
   "metadata": {},
   "source": [
    "Finally, so far you got a powerful model capable to use it to predict in new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1db38",
   "metadata": {},
   "source": [
    "**Step22** \n",
    "\n",
    "**Predict on a new sample** Process the image `challenge1.jpg` and try to dechiper what is the letter in the image using your best model. Be aware that your model gives you numeric results, however you should convert this result in a proper output of letters (use lowercase letters).\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ca85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the image (do not modify this line of code)\n",
    "img_challenge1 = cv2.imread('challenge1.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Plot the image (do not modify the following 2 lines of code)\n",
    "plt.imshow(img_challenge1)\n",
    "plt.show()\n",
    "\n",
    "# Process the data\n",
    "\n",
    "# Predict in this data using your best model\n",
    "prediction_challenge1 = ...\n",
    "\n",
    "\n",
    "# Decoding result. This should be the string representation of the output generated by your model.\n",
    "result_challenge1 = ...\n",
    "\n",
    "# The following code is used by the autograder. Do not modify it.\n",
    "step22_sol = (result_challenge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d3084-6c7e-4d2d-aaf9-04a1d2e59cce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe3a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "otter": {
   "OK_FORMAT": false,
   "tests": {
    "step00": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step00\"\npoints = 1\n\n@test_case(points=1, hidden=False, \n    failure_message=\"One or more files are missing!\")\ndef test_q0(step0_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertTrue(step0_data)\n",
    "step01": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step01\"\npoints = 1\n\n@test_case(points=1, hidden=False)\ndef test_q1(step1_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    # Check dimensions    \n    TC.assertEqual(step1_sol, (7172, 785))\n",
    "step02": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step02\"\npoints = 1\n\n@test_case(points=1, hidden=False)\ndef test_q2(step2_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    # Checking the numnber of classes\n    a = (step2_sol[0] == 1)\n    b = (step2_sol[1] == 23)\n    c = (step2_sol[2] == 1)\n    d = (step2_sol[3] == 23)\n    TC.assertTrue(a)\n    TC.assertTrue(b)\n    TC.assertTrue(c)\n    TC.assertTrue(d)\n",
    "step03": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step03\"\npoints = 1\n\n@test_case(points=1, hidden=False)\ndef test_q3(step3_sol):    \n    import unittest as UT\n    TC = UT.TestCase()    \n    unique_values_1 = (step3_sol[0] == 2)\n    unique_values_2 = (step3_sol[1] == 2)\n    TC.assertTrue(unique_values_1)\n    TC.assertTrue(unique_values_2)\n",
    "step04": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step04\"\npoints = 0\n\n@test_case(points=0, hidden=False)\ndef test_q4():\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertEqual(1 ,1)\n",
    "step05": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step05\"\npoints = 5\n\n@test_case(points=5, hidden=False)\ndef test_q5(step5_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    # Check if model is sequential\n    TC.assertTrue(\"sequential\" in step5_sol.get_config()['name'])\n    # Check if model has Dense as second layer and check forward layers\n    TC.assertTrue(all('Dense' in layer for layer in [step5_sol.get_config()['layers'][x*3+1]['class_name'] for x in range(20)]))\n    # Check number of 100 units in dense layer and check forward layers\n    TC.assertTrue(all(100 == layer for layer in [step5_sol.get_config()['layers'][x*3+1]['config']['units']  for x in range(20)]))\n    # Check the kernel initialization equal He and check forward layers\n    TC.assertTrue(all('He' in layer for layer in [step5_sol.get_config()['layers'][x*3+1]['config']['kernel_initializer']['class_name'] for x in range(20)]))\n    # Check if next layer is a batchnormalization and check forward layers\n    TC.assertTrue(all('BatchNormalization' in layer for layer in [step5_sol.get_config()['layers'][x*3+2]['class_name'] for x in range(20)]))\n    # Check the activation function and check forward layers\n    TC.assertTrue(all('swish' in layer for layer in [step5_sol.get_config()['layers'][x*3+3]['config']['activation'] for x in range(20)]))\n    # Check if last layer has an activation softmax function\n    TC.assertTrue('softmax' in step5_sol.get_config()['layers'][-1]['config']['activation'])\n    # Check if last layer has 24 units\n    TC.assertTrue(24 == step5_sol.get_config()['layers'][-1]['config']['units'])\n",
    "step06": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step06\"\npoints = 5\n\n@test_case(points=5, hidden=False)\ndef test_q6(step6_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    # Check if model is sequential\n    TC.assertTrue(\"sequential\" in step6_sol.get_config()['name'])\n    \n    \n    TC.assertTrue(any('Dense' in layer for layer in [step6_sol.get_config()['layers'][x]['class_name'] for x in range(1,10)]))\n    # Check if model has AlphaDropout as third layer and after every two layers [3,6,7,9 ...]\n    TC.assertTrue(any('AlphaDropout' in layer for layer in [step6_sol.get_config()['layers'][x]['class_name']  for x in range(1,10)]))\n    \n",
    "step07": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step07\"\npoints = 2\n\n@test_case(points=2, hidden=False)\ndef test_q7(step7_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    # Check the proper loss function\n    TC.assertTrue('categorical_crossentropy' in step7_sol[0])\n    TC.assertTrue('categorical_crossentropy' in step7_sol[1])\n    # Check optimizer\n    TC.assertTrue('Nadam' in step7_sol[2])\n    TC.assertTrue('Nadam' in step7_sol[3])\n",
    "step08": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step08\"\npoints = 2\n\n@test_case(points=2, hidden=False)\ndef test_q8(step8_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    # We create a standardscaler instance\n    TC.assertAlmostEqual(step8_sol[0],0,delta=0.05)\n    TC.assertAlmostEqual(step8_sol[1],0,delta=0.05)\n    TC.assertAlmostEqual(step8_sol[2],1,delta=0.05)\n    TC.assertAlmostEqual(step8_sol[3],1,delta=0.05)\n",
    "step09": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step09\"\npoints = 3\n\n@test_case(points=3, hidden=False)\ndef test9_q(step9_sol):\n    import unittest as UT\n    import pickle\n    from tensorflow.keras.callbacks import EarlyStopping\n    TC = UT.TestCase()\n    # Load history for model 1\n    history_name1 = \"./history1\"\n    with open(history_name1, \"rb\") as file_pi:\n        loaded_history1 = pickle.load(file_pi)\n    # Load history for model 2\n    history_name2 = \"./history2\"\n    with open(history_name2, \"rb\") as file_pi:\n        loaded_history2 = pickle.load(file_pi)\n    ans1 = (len(loaded_history1[\"accuracy\"]) <= 100)\n    ans2 = (len(loaded_history2[\"accuracy\"]) <= 100)\n    ans3 = (isinstance(step9_sol[2], EarlyStopping))\n    TC.assertTrue(ans1)\n    TC.assertTrue(ans2)\n    TC.assertTrue(ans3)\n",
    "step10": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step10\"\npoints = 2\n\n@test_case(points=2, hidden=False)\ndef test10_q(step10_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    ans1 = (step10_sol[1] >= 0.5)\n    ans2 = (step10_sol[3] >= 0.5)\n    TC.assertTrue(ans1)\n    TC.assertTrue(ans2)\n",
    "step11": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step11\"\npoints = 0\n\n@test_case(points=0, hidden=False)\ndef test11_q(step11_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertTrue(1==1)\n",
    "step12": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step12\"\npoints = 3\n\n@test_case(points=3, hidden=False)\ndef test12_q(step12_sol,step10_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    ans1 = (step12_sol[0].shape[1] == 24)\n    TC.assertTrue(ans1)\n    ans2 = (step12_sol[1] > step10_sol[3])\n    TC.assertTrue(ans2)\n\n",
    "step13": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step13\"\npoints = 2\n\n@test_case(points=2, hidden=False)\ndef test13_q(step13_sol):\n    import unittest as UT\n    import pickle\n    from tensorflow.keras.callbacks import ReduceLROnPlateau\n    TC = UT.TestCase()\n    # Load history for model 1\n    history_perf1_name = \"./history1_perf\"\n    with open(history_perf1_name, \"rb\") as file_pi:\n        loaded_history1_perf = pickle.load(file_pi)\n    ans1 = (len(loaded_history1_perf[\"accuracy\"]) <= 100)\n    ans2 = (isinstance(step13_sol[1], ReduceLROnPlateau))\n    TC.assertTrue(ans1)    \n    TC.assertTrue(ans2)\n\n",
    "step14": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step14\"\npoints = 1\n\n@test_case(points=1, hidden=False)\ndef test14_q(step14_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    ans = (step14_sol[1] >= 0.5)\n    TC.assertTrue(ans)\n",
    "step15": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step15\"\npoints = 0\n\n@test_case(points=0, hidden=False)\ndef test15_q():\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertTrue(1==1)\n",
    "step16": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step16\"\npoints = 2\n\n@test_case(points=2, hidden=False)\ndef test16_q(step16_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    # Check dimensions\n    TC.assertEqual(step16_sol[0].shape[1:],(28,28,3))\n    TC.assertEqual(step16_sol[1].shape[1:],(28,28,3))\n",
    "step17": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step17\"\npoints = 2\n\n@test_case(points=2, hidden=False)\ndef test17_q(step17_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n\n    # Check if the model is from MobileNet\n    layer_names_step17_test = []\n    for l in range(len(step17_sol.get_config()['layers'])):\n        layer_names_step17_test.append(step17_sol.get_config()['layers'][l]['config']['name'])\n    TC.assertTrue('MobilenetV3small' in layer_names_step17_test)\n\n    # Check layers have frozen weights over the base model layers\n    for main_layer in range(len(step17_sol.get_config()['layers'])):\n        if \"MobilenetV3small\" == step17_sol.get_config()['layers'][main_layer]['config']['name']:\n            for mb_layer in range(1,20):\n                testing_trainable_step17_test = step17_sol.get_config()['layers'][main_layer]['config']['layers'][mb_layer]['config']['trainable']\n                if testing_trainable_step17_test == True:\n                    break\n    TC.assertTrue(False == testing_trainable_step17_test)\n\n    # Check the input shape modified by the factor\n    TC.assertTrue(step17_sol.input_shape[1:],(56,56,3))\n\n    # Check if the final layer of the new model contains 24 classes and the activation is softmax\n    TC.assertEqual(step17_sol.get_config()['layers'][-1]['config']['units'],24)\n    TC.assertTrue('softmax' in step17_sol.get_config()['layers'][-1]['config']['activation'])\n",
    "step18": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step18\"\npoints = 2\n\n@test_case(points=2, hidden=False)\ndef test18_q(step18_sol):\n    import unittest as UT\n    import tensorflow as tf\n    TC = UT.TestCase()\n\n    # Check if the schedule is created from ExponentialDecay\n    TC.assertTrue(isinstance(step18_sol[0],tf.keras.optimizers.schedules.ExponentialDecay))\n\n    # Check the compile contains Nadam\n    TC.assertTrue('Nadam' in step18_sol[1].optimizer.get_config()['name'])\n\n    # Check the compile uses loss of categorical_crossentropy\n    TC.assertTrue('categorical_crossentropy' in step18_sol[1].loss)\n",
    "step19": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step19\"\npoints = 2\n\n@test_case(points=2, hidden=False)\ndef test19_q(step19_sol):\n    import unittest as UT\n    import pickle\n    from tensorflow.keras.callbacks import EarlyStopping\n    TC = UT.TestCase()\n\n    # Load history for model 1\n    history_namemb = \"./historymb\"\n    with open(history_namemb, \"rb\") as file_pi:\n        loaded_historymb = pickle.load(file_pi)\n\n    # Check the length of training\n    TC.assertTrue(len(loaded_historymb[\"accuracy\"]) <= 100)\n    TC.assertTrue(isinstance(step19_sol[1], EarlyStopping))\n",
    "step20": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step20\"\npoints = 1\n\n@test_case(points=1, hidden=False)\ndef test20_q(step20_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    ans = (step20_sol[1] >= 0.5)\n    TC.assertTrue(ans)\n",
    "step21": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step21\"\npoints = 2\n\n@test_case(points=2, hidden=False)\ndef test21_q(step21_sol):\n    import unittest as UT\n    import numpy as np\n    TC = UT.TestCase()\n\n    # Create a dictionary of results  \n    dictionary_solution_21 = {0:\"a\",   \n                  1:\"b\",    \n                  2:\"c\",    \n                  3:\"d\",    \n                  4:\"e\",    \n                  5:\"f\",    \n                  6:\"g\",    \n                  7:\"h\",    \n                  8:\"i\",    \n                  9:\"k\",    \n                  10:\"l\",   \n                  11:\"m\",   \n                  12:\"n\",   \n                  13:\"o\",   \n                  14:\"p\",   \n                  15:\"q\",   \n                  16:\"r\",   \n                  17:\"s\",   \n                  18:\"t\",   \n                  19:\"u\",   \n                  20:\"v\",   \n                  21:\"w\",   \n                  22:\"x\",   \n                  23:\"y\"}   \n    prediction_class_solution = []                                                        \n    for letter in range(step21_sol[1].shape[0]):                               \n        prediction_class_solution.append(np.argmax(step21_sol[1][letter]))              \n    matches_letter_solution = {}                                                          \n    for letter in range(24):                                                     \n        matches_solution = 0                                                              \n        for row in range(len(step21_sol[0])):                                      \n            if (step21_sol[0][row] == letter) and (step21_sol[0][row] == prediction_class_solution[row]):    \n                matches_solution += 1                                                                    \n        matches_letter_solution[letter] = matches_solution/np.count_nonzero(step21_sol[0] == letter)                \n    # Get the hardest letters\n    complex_letter_possible_solutions = [kv[0] for kv in matches_letter_solution.items() if kv[1] == min(matches_letter_solution.values())]\n    complex_letter_possible_solutions = [dictionary_solution_21[i] for i in complex_letter_possible_solutions]\n    # Get the easiest letters\n    easiest_letter_possible_solutions = [kv[0] for kv in matches_letter_solution.items() if kv[1] == max(matches_letter_solution.values())]\n    easiest_letter_possible_solutions = [dictionary_solution_21[i] for i in easiest_letter_possible_solutions]\n    \n    ans1 = (step21_sol[2] in complex_letter_possible_solutions)\n    ans2 = (step21_sol[3] in easiest_letter_possible_solutions)\n    # Check if answers are in the possible solutions\n    TC.assertTrue(ans1)\n    TC.assertTrue(ans2)\n    \n",
    "step22": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step22\"\npoints = 2\n\n@test_case(points=2, hidden=False)\ndef test22_q(step22_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    ans = (step22_sol == 'k')\n    # Check correct letter\n    TC.assertTrue(ans)\n"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
